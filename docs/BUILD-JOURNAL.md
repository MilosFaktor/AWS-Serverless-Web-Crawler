## ğŸ”¹ CI â€“ Step 1: Source (Pull Request Event via AWS CodeStar Connection)

### What I Did
- Connected CodePipeline to GitHub using **AWS CodeStar Connections** (GitHub App authentication).
- Created the Source stage in the CI pipeline to trigger from **pull request events** instead of direct pushes.
- Set the **destination branch filter** to `main` (target branch), not the `feature/*` branches.
- Configured event triggers for:
  - **Created** (when a PR is opened)
  - **Updated** (when commits are pushed to the PRâ€™s source branch)
- Left **Closed** unselected (not needed for my flow).

### Key Skill Learnings
- **PR Destination Filtering** â€“ PR triggers filter on the *target branch*, not the *source branch*. My first attempt was `feature/*`, which didnâ€™t work until I switched to `main`.
- **Continuous PR Updates** â€“ Once a PR is open, any new commit to the source branch automatically triggers the pipeline without creating a new PR.
- **Granular Event Control** â€“ Selecting only â€œCreatedâ€ and â€œUpdatedâ€ keeps the pipeline lean while still supporting rapid iteration.

### Challenges & How I Solved Them
- **Main Struggle:** Pipeline not triggering on feature branch commits. Thought CodePipeline could filter on source branches directly â€” it canâ€™t.
- **Fix:** Set destination branch filter to `main` â†’ worked immediately.
- **Extra Lesson:** Understanding PR event filtering will save hours of debugging in future CI/CD setups.

### Outcome
- CI pipeline starts automatically for any new or updated PR targeting `main`.
- I can commit freely to a feature branch while a PR is open, knowing CodePipeline will rebuild without manual intervention.


## ğŸ”¹ CI â€“ Step 2: Build & Package (CodeBuild)

### What I Did
- Set up a **build-only** CodeBuild stage.  
- `buildspec.yml` stays minimal and calls a shell script that:
  1. Validates the SAM/CloudFormation template.
  2. Runs `sam build` to stage artifacts under `.aws-sam/build/`.
- Used **CodeBuild Standard (AL2 v5)** image (Python 3.11, Node 20, SAM CLI preinstalled).
- Exported only the **needed artifacts** (template + build outputs) for downstream stages and for the CD pipeline via the artifact S3 bucket.

---

### Why I Split Build/Package and Deploy
- I wanted **one deploy script** and **one SAM template** for both environments (**Dev** and **Prod**), with behavior controlled by an environment variable (`STAGE=dev|prod`).
- Keeping deploy separate lets me inject **env-specific settings** at runtimeâ€”without rebuilding artifacts.
- Stack names, params, and regions come from `STAGE` (and/or `samconfig.toml` profiles), e.g.:
  - `serverless-app-crawler-sam-dev`
  - `serverless-app-crawler-sam-prod`
- **Result**: same artifacts, same script, different stack targets â€” clean, DRY, and reproducible.

---

### ğŸ§  Key Skill Learnings
- **Artifacts = handoff contract** to later stages and CD; list only whatâ€™s required.
- Path discovery of `.aws-sam/build/` avoids â€œfile not foundâ€ in later steps.
- Runtimes preinstalled â†’ removing explicit installs reduced ~30s build time.
- Caching not helpful for this SAM flow; clean rebuilds keep template/code in sync.

---

### ğŸ› ï¸ Challenges & Fixes
- **Which files to export?** Iterated with `logs/ls -R` and trimmed to essentials.
- Over-configuring runtimes slowed builds â†’ relied on the standard image.
- **Packaging vs Deploy**: kept packaging logic with deploy (`sam deploy`) so env-specific uploads/params are driven by `STAGE`/profiles, not the build step.

---

### âœ… Outcome
- Lean, fast **Build & Package** that produces one set of artifacts reused for:
  - CI Step 3 (Dev deploy)
  - CI Step 4 (tests via stack outputs)
  - CD pipeline (Prod deploy)  
  â€” ensuring Prod runs the **exact bits** validated in CI.


## ğŸ”¹ CI Pipeline â€“ Step 3: Deploy to Dev

### ğŸ“Œ What I Did
In this stage, the CI pipeline takes the **packaged artifacts from Step 2** and deploys them into the **development stack** using AWS SAM.  

This stage is driven by:  
- A **dedicated buildspec file** for deployment.  
- A matching `deploy.sh` script.  

**Buildspec Flow**:
1. `pre_build` â†’ Logs the target environment.
2. `build` â†’ Runs the deployment script.
3. `post_build` â†’ Logs completion.

---

### ğŸ›  `deploy.sh` Script Highlights
- Ensures itâ€™s in the correct working directory (`serverless-app-sam/`).
- Resolves the **stack name dynamically** from the `samconfig.toml` environment section.
- Checks the current CloudFormation **stack status** before deployment.
- Handles **failed/incomplete stacks** (`ROLLBACK_COMPLETE`, `CREATE_FAILED`, etc.) by deleting them and waiting for completion.
- Runs `sam deploy` using the **env-specific settings** in `samconfig.toml`.
- If deployment fails, checks if the stack is simply **already up to date** and allows the pipeline to continue.
- Generates **two versions of stack outputs**:
  - Full JSON
  - Simplified key-value JSON (for downstream stages)
- Publishes all required files as **DeployArtifacts** for later stages like testing and production deploy.

---

### ğŸš§ Challenges I Overcame
- **IAM Permissions Roadblock**:  
  - This was the single biggest blocker in this stage.  
  - CodeBuild needs permissions for every AWS service your SAM app interacts with: CloudFormation, Lambda, DynamoDB, S3, API Gateway, etc.  
  - Without the right IAM policy, deployment fails mid-pipeline.  
  - Initially, I troubleshot **one missing permission at a time** â€” slow and frustrating.

- **Solution**:  
  - Use **different IAM roles** for development and production:
    - **Dev IAM Role** â†’ Loose permissions for fast iteration.
    - **Prod IAM Role** â†’ Strict least-privilege before go-live.
  - Learned to **auto-handle ROLLBACK** and failed stack states so they donâ€™t block deployments.
  - Learned to detect **"no changes"** and continue without failing the pipeline.
  - Fixed artifact paths so files in `serverless-app-sam/` are correctly picked up by CodePipeline.

---

### ğŸ§  Key Learnings
- If something isnâ€™t working in AWS, **check IAM policies or security groups first** â€” most common blockers.
- Keep **build and deploy** as separate stages so the same deploy script can be reused for multiple environments just by changing `$Environment`.
- Using **different IAM roles** for dev and prod balances speed and security.
- Always store outputs in both **full** and **simplified** formats for easier test automation.

---

### âœ… Outcome
- Fully deployed **Dev stack** with clean CloudFormation outputs stored as artifacts.
- Deployment resilient to failed states.
- IAM issues documented and resolved.
- Workflow ready to extend to **Prod deployment** with minimal changes.


## ğŸ”¹ CI Pipeline â€“ Step 4: Integration Testing

### ğŸ¯ Purpose of This Stage
This is the **full end-to-end verification step** for the pipeline.  
By this point, the **Dev stack** has been freshly deployed in **Step 3**, so this stage confirms that the application **actually works as intended** â€” not just that CloudFormation deployed it.

This stage runs the **entire serverless flow**:

1. Invoke **Initiator Lambda**.
2. Let it publish a message to **SQS**.
3. Allow SQS to trigger **Crawler Lambda**.
4. Crawler Lambda scrapes the target website and stores results in **DynamoDB**.
5. Confirm DynamoDB has at least **1 record** from the test run.

This is not a unit test â€” itâ€™s a **real functional test** across multiple AWS services, ensuring all components communicate correctly.

---

### ğŸ›  What Actually Happens in This Stage

The stage is executed by **CodeBuild** using a **dedicated buildspec.yml** for integration testing.  
That buildspec calls `integration_test.sh`, which performs:

#### 1. Load stack outputs from artifacts
- Uses `cfn-outputs.json` and `cfn-outputs-simple.json` from Step 3.
- Extracts Lambda ARN, SQS URL, and DynamoDB table name dynamically â€” **no hardcoding**.

#### 2. Extract resource identifiers with `jq`
- `InitiatorFunction` â†’ Lambda ARN.
- `CrawlerQueueUrl` â†’ SQS queue URL.
- `VisitedTableName` â†’ DynamoDB table name.

#### 3. Invoke Initiator Lambda
- Sends test event (`event.json`) to Lambda.
- **Key Fix:** Use `fileb://` instead of `file://` for payload to avoid JSON parsing issues in AWS CLI.

#### 4. Wait before polling SQS
- Added a **30-second initial delay** before polling.
- Prevents false negatives due to async delays in Lambda + SQS.

#### 5. Poll SQS until empty
- Checks both **visible** and **in-flight** messages.
- Declares queue â€œdoneâ€ only if empty for **3 consecutive polls** (10s apart).

#### 6. Scan DynamoDB table
- Runs `aws dynamodb scan` and saves to `response_dynamodb_table.json`.
- Extracts record count to determine pass/fail.

#### 7. Pass/Fail criteria
- âœ… PASS â†’ At least 1 record in DynamoDB.
- âŒ FAIL â†’ No records found or SQS timeout.

---

### ğŸš§ Challenges I Overcame

- **Artifacts Not Passing Through**  
  - First run failed because `cfn-outputs.json` wasnâ€™t in Step 3 artifacts.
  - CodePipeline doesnâ€™t auto-share files â€” must be explicitly listed.

- **Payload Format Issue**  
  - Missing `b` in `fileb://` caused repeated Lambda invocation failures.

- **Timing Issues**  
  - Without delay, poller started too early and missed messages.
  - Added **3-empty-check rule** to handle intermittent empty queue states.

---

### ğŸ§  Key Learnings
- Integration tests need **both correct artifacts and timing** to work reliably.
- Async AWS services like **SQS + Lambda** require **deliberate wait logic**.
- `fileb://` vs `file://` in AWS CLI is a **tiny but critical** difference.
- Always verify **artifact passing** between stages before complex automation.

---

### âœ… Outcome
This stage now:
- Dynamically reads resource names from stack outputs (**no hardcoding**).
- Waits properly for async processing to finish.
- Produces **detailed JSON artifacts** for Lambda responses & DynamoDB results.
- Serves as a **reliable pass/fail gate** before production deployment.


## ğŸ”¹ CI â€“ Step 5: Manual Approval, SNS Notification & GitHub Status Check Enforcement

### ğŸ›  What I Did
- Added a **Manual Approval** action at the end of the CI pipeline.
- Linked it to an **SNS topic** to send an email containing:
  - ğŸ“ Link to the GitHub Pull Request (PR) for review/merge.
  - ğŸ“ Link to the CodePipeline approval page.
- Added a **post-approval CodeBuild job** in the same stage to run `post_approval.sh` which:
  - Posts to the **GitHub Status API** with `"state": "success"`.
  - Uses a **GITHUB_TOKEN** stored in **AWS Secrets Manager** for authentication.
  - Passes `CODEBUILD_RESOLVED_SOURCE_VERSION` to target the correct commit.

---

### âš™ï¸ How It Behaves
1. CI runs through:
   - **Build**
   - **Deploy-to-Dev**
   - **Integration Tests**
2. If all pass, pipeline **pauses** at the **Manual Approval** step and sends the **SNS email**.
3. After approval:
   - The **post-approval CodeBuild job** runs.
   - Posts the **success status** to GitHub.
4. **GitHub Branch Protection** requires this status check before merges.

âœ… **Merge Button is Locked Until:**
- Pipeline finishes all prior steps.
- Manual approval in CodePipeline is given.
- Status check from CodeBuild posts successfully.

---

### ğŸ§  Key Learnings
- Manual approval in CodePipeline alone is only a **soft safeguard** â€” without a GitHub rule, merges could bypass it.
- Linking a post-approval CodeBuild job to **GitHubâ€™s Status API** and marking it **required** in branch protection makes it a **hard gate**.
- **Secrets Manager** securely passes tokens to CodeBuild without exposing them in logs.
- SNS notification + status check = **visibility (email)** + **enforcement (GitHub lock)**.

---

### ğŸš§ Challenges & Fixes
- Fixed file path issues when calling `post_approval.sh` in CodeBuild (`chmod +x ./codepipeline/scripts/post_approval.sh`).
- Corrected GitHub repo name in API call (initially pointed to the wrong repo).
- Granted `sns:Publish` permission to the CodePipeline role.
- Verified SNS subscription to ensure email notifications work.

---

### âœ… Outcome
- CI is **fully automated** up to an **enforced approval checkpoint**.
- **Approval â†’ Status Check â†’ Merge** = clean, tested build flows into CD.
- Merge without approval is **blocked by GitHub** until status check passes.
- SNS email provides **quick links** to the PR and approval screen for convenience.



## ğŸ”¹ GitHub Flow + Pipeline Merge Protection â€“ Avoiding Unwanted Production Merges

### ğŸ›  What I Did
- Adopted the **GitHub Flow** branching model:
  - `main` â†’ always production-ready, protected.
  - `feature/*` â†’ all new work and changes are developed here.
- Added **branch protection rules** on `main` (enforced even for admins).
- Configured rules so:
  - âŒ Direct commits to `main` are blocked.
  - âœ… All changes must come via a **Pull Request** from `feature/*` â†’ `main`.
  - âœ… PR merges are only allowed if **all required status checks** pass.
- Integrated **AWS CodePipeline** with the **GitHub Status API**:
  - Added a **post-approval step** at the end of the pipeline that sends a `"success"` status to the relevant commit in GitHub.
  - Used **AWS Secrets Manager** to securely store the GitHub token for authentication.
  - Configured the GitHub branch rule so the **merge button remains locked** until the pipeline posts a green status.

---

### âš™ï¸ How It Behaves
1. Developer pushes changes to a `feature/*` branch.
2. A PR is opened to merge into `main`.
3. **CodePipeline** runs:
   - Build
   - Deploy-to-Dev
   - Tests
   - Manual Approval (SNS notification sent for visibility)
4. Upon manual approval:
   - **Post-approval script** runs, posting `"success"` to the GitHub Status API for that commit.
5. Only after the status check passes does GitHub allow the merge.
6. Merging into `main` triggers the **CD pipeline** to deploy to production.

---

### ğŸ§  Key Learnings
- **GitHub Flow** is lightweight, simple, and works perfectly with CI/CD.
- Protecting `main` â€” even for admins â€” removes the temptation to â€œjust push it.â€
- **Branch protection + pipeline-driven status checks** = hard security gate for production.
- SNS + Manual Approval still adds human oversight before status check passes.
- This pattern **prevents accidental merges, untested code, or hotfix mistakes** from ever reaching production.

---

### âœ… Outcome
Every deployment to `main` now follows:

